{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from shutil import copyfile\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from collections import namedtuple\n",
    "from sklearn.cluster import KMeans\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_Model = namedtuple('Model', ['name', 'layer', 'layer_output_size'])\n",
    "\n",
    "models_dict = {\n",
    "    'resnet-18': _Model('resnet18', 'avgpool', 512),\n",
    "    'alexnet': _Model('alexnet', 'classifier', 4096),\n",
    "    'vgg-11': _Model('vgg11', 'classifier', 4096),\n",
    "    'densenet': _Model('densenet', 'classifier', 1024),\n",
    "    'efficientnet_b0': _Model('efficientnet_b0', '_avg_pooling', 1280),\n",
    "    'efficientnet_b1': _Model('efficientnet_b1', '_avg_pooling', 1280),\n",
    "    'efficientnet_b2': _Model('efficientnet_b2', '_avg_pooling', 1408),\n",
    "    'efficientnet_b3': _Model('efficientnet_b3', '_avg_pooling', 1536),\n",
    "    'efficientnet_b4': _Model('efficientnet_b4', '_avg_pooling', 1792),\n",
    "    'efficientnet_b5': _Model('efficientnet_b5', '_avg_pooling', 2048),\n",
    "    'efficientnet_b6': _Model('efficientnet_b6', '_avg_pooling', 2304),\n",
    "    'efficientnet_b7': _Model('efficientnet_b7', '_avg_pooling', 2560),\n",
    "}\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        img = Image.open(image_path)\n",
    "        image = img.resize((224, 224))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return str(image_path), image\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        image_paths, images = zip(*data)\n",
    "        # Stack images to create a batch        \n",
    "        images = torch.stack(images)\n",
    "        return list(image_paths), images\n",
    "\n",
    "\n",
    "\n",
    "# create a class for extracting embeddings\n",
    "class Embeddings:\n",
    "    def __init__(self,\n",
    "                 model_name: str ='resnet-18',\n",
    "                 cuda: bool =False,\n",
    "                 tensor: bool = True\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param model_name: name of the model to be used for extracting embeddings (default: 'resnet-18') \n",
    "            Other available models: 'alexnet', 'vgg-11', 'densenet', 'efficientnet_b0', 'efficientnet_b1', \n",
    "            'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7'\n",
    "        :param cuda: whether to use cuda or not\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.layer = models_dict[model_name].layer\n",
    "        self.layer_output_size = models_dict[model_name].layer_output_size\n",
    "        self.model, self.extraction_layer = self.get_model_and_layer()\n",
    "        self.model.eval()\n",
    "        self.cuda = cuda\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"\n",
    "        :param image_path: path to the image\n",
    "        :return: image\n",
    "        \"\"\"\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize((224, 224))\n",
    "        return img\n",
    "\n",
    "    def get_model_and_layer(self):\n",
    "        \"\"\"\n",
    "        :return: model and layer\n",
    "        \"\"\"\n",
    "        model = models.__dict__[models_dict[self.model_name].name](pretrained=True)\n",
    "        layer = getattr(model, self.layer)\n",
    "        return model, layer\n",
    "    \n",
    "\n",
    "    def get_image_embedding(self, \n",
    "                            image_path: Union[List[str], str], \n",
    "                            tensor: bool = None, \n",
    "                            cuda: bool = None):\n",
    "        \"\"\"\n",
    "        :param image_path: path to the image\n",
    "        :return: image embedding\n",
    "        \"\"\"\n",
    "        if not tensor:\n",
    "            tensor = self.tensor\n",
    "        if not cuda:\n",
    "            cuda = self.cuda\n",
    "            \n",
    "        img2vec = Img2Vec(cuda=cuda)\n",
    "\n",
    "        img = self.load_image(image_path)\n",
    "        return img2vec.get_vec(img)\n",
    "        \n",
    "    def generate_embedding(self, \n",
    "                           images_path: Union[List[str], str],\n",
    "                           dir_embeddings_output: str,\n",
    "                           embedding_dimension: int = 512,\n",
    "                           batch_size: int = 100):\n",
    "        \n",
    "        if isinstance(images_path, str):\n",
    "            image_paths = [os.path.join(images_path, image) for image in os.listdir(images_path)]\n",
    "        else:\n",
    "            image_paths = images_path\n",
    "\n",
    "        if not os.path.exists(dir_embeddings_output):\n",
    "            os.makedirs(dir_embeddings_output)\n",
    "\n",
    "        batch_size = min(batch_size, len(image_paths))\n",
    "        \n",
    "        labels = [0] * len(image_paths)\n",
    "        n_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "        print(\"Total number of images: \", len(image_paths))\n",
    "        print(\"Number of batches: \", n_batches)\n",
    "\n",
    "        img2vec = Img2Vec(cuda=self.cuda)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        dataset = ImageDataset(image_paths, transform=transform)  # Apply transformations if needed\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=dataset.collate_fn)\n",
    "        to_pil = ToPILImage()\n",
    "\n",
    "\n",
    "        def process_image(image):\n",
    "            pil_image = to_pil(image)\n",
    "            # Apply your functions here\n",
    "            return pil_image\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            for i, (image_paths, images) in tqdm.tqdm(enumerate(dataloader), total=len(image_paths), desc='Progress', ncols=100, ):\n",
    "                pil_images = list(executor.map(process_image, images))\n",
    "                vec = img2vec.get_vec(pil_images)\n",
    "                print(i, vec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahmoudouf/Documents/Github/ZenSVI/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mahmoudouf/Documents/Github/ZenSVI/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  5056\n",
      "Number of batches:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|                                                            | 0/5056 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "emb = Embeddings(model_name=\"resnet-18\", cuda=False)\n",
    "emb.generate_embedding(\n",
    "    \"./images/\",\n",
    "    \"./embeddings/\",\n",
    "    batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
